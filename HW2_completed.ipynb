{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lwNHyfGYi2-p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "# Function to create the rotation matrix for RoPE embeddings\n",
        "def generate_rotation_matrix(seq_length, embed_dim):\n",
        "    # [INSTRUCTION] Complete this function to generate a rotation matrix for RoPE embeddings.\n",
        "    # The matrix rotates queries and keys based on sequence position and embedding dimension.\n",
        "    # Hint: Consider using the provided exponential decay formula for theta and apply trigonometric rotations.\n",
        "    pass  # Implement your function based on the hint\n",
        "    # Initialize the rotation matrix R\n",
        "    R = torch.zeros((seq_length, embed_dim, embed_dim))\n",
        "\n",
        "    # Create positions tensor from 1 to seq_length\n",
        "    positions = torch.arange(1, seq_length + 1).unsqueeze(1)  # shape: (seq_length, 1)\n",
        "\n",
        "    # Calculate theta values based on the formula\n",
        "    theta = positions / torch.pow(10000, torch.arange(0, embed_dim, 2).float() / embed_dim)\n",
        "\n",
        "    # Calculate sin and cos values\n",
        "    sin_values = torch.sin(theta.unsqueeze(-1))  # shape: (seq_length, embed_dim // 2, 1)\n",
        "    cos_values = torch.cos(theta.unsqueeze(-1))  # shape: (seq_length, embed_dim // 2, 1)\n",
        "\n",
        "    # Expand sin and cos values to match the dimensions of R\n",
        "    sin_expanded = sin_values.repeat(1, 1, embed_dim // 2)  # shape: (seq_length, embed_dim // 2, embed_dim // 2)\n",
        "    cos_expanded = cos_values.repeat(1, 1, embed_dim // 2)  # shape: (seq_length, embed_dim // 2, embed_dim // 2)\n",
        "\n",
        "    for i in range(embed_dim // 2):\n",
        "        R[:, 2 * i, 2 * i] = cos_expanded[:, i, i]\n",
        "        R[:, 2 * i, 2 * i + 1] = -sin_expanded[:, i, i]\n",
        "        R[:, 2 * i + 1, 2 * i] = sin_expanded[:, i, i]\n",
        "        R[:, 2 * i + 1, 2 * i + 1] = cos_expanded[:, i, i]\n",
        "\n",
        "    return R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lR5AJFpqjF6u"
      },
      "outputs": [],
      "source": [
        "# Custom RMS Normalization layer\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, layer_dims, epsilon=1e-8):\n",
        "        super().__init__()\n",
        "        self.register_parameter(\"gamma\", nn.Parameter(torch.ones(layer_dims)))\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, tensor_input):\n",
        "        # [INSTRUCTION] Complete the forward method to normalize the input tensor using RMSNorm.\n",
        "        # Compute the root mean square of the tensor values and use it to normalize the tensor_input.\n",
        "        # Consider dimensions appropriately for batch processing and do not forget to add epsilon for numerical stability.\n",
        "        pass  # Implement your normalization logic here\n",
        "        mean_rms = torch.sqrt(torch.mean(tensor_input**2, dim=tuple(range(1, len(tensor_input.shape))), keepdim=True) + self.epsilon)\n",
        "\n",
        "        normalized = tensor_input / mean_rms\n",
        "\n",
        "        return normalized * self.gamma.to(normalized.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bPGqRdIfjF9X"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "# Attention head with RoPE embeddings and masking\n",
        "class AttentionHeadWithRoPE(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super().__init__()\n",
        "        self.params = params\n",
        "        # [INSTRUCTION] Initialize linear layers for queries, keys, and values.\n",
        "        # Each should transform input data to the same embedding dimensionality, and none should include bias terms.\n",
        "        pass  # Initialize query, key, and value linear transformations here\n",
        "\n",
        "        # [INSTRUCTION] Retrieve the rotation matrix generated for RoPE embeddings, ensuring it is moved to the correct device.\n",
        "        pass  # Load the rotation matrix here\n",
        "\n",
        "        embed_dim = params['embed_dim']\n",
        "\n",
        "        # Initialize linear layers for queries, keys, and values\n",
        "        self.query_linear = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.key_linear = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.value_linear = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "        self.register_buffer('rotation_matrix', params['rotation_matrix'].to(params['device']))\n",
        "\n",
        "    def forward(self, data_input, return_attention=False):\n",
        "        batch_size, seq_len, embed_dim = data_input.shape\n",
        "        # [INSTRUCTION] Transform the input data into queries, keys, and values using the respective linear layers.\n",
        "        pass  # Transform data_input into queries, keys, and values here\n",
        "\n",
        "        # [INSTRUCTION] Apply the RoPE rotation to both queries and keys. Think about the tensor shapes for matrix multiplication.\n",
        "        pass  # Apply RoPE rotation to queries and keys here\n",
        "\n",
        "        # [INSTRUCTION] Compute scaled dot-product attention. Remember to handle masking if 'return_attention' is true.\n",
        "        # Optionally return attention weights for visualization or debugging.\n",
        "        pass  # Compute attention and optionally return attention weights\n",
        "        batch_size, seq_len, embed_dim = data_input.shape\n",
        "\n",
        "        queries = self.query_linear(data_input)\n",
        "        keys = self.key_linear(data_input)\n",
        "        values = self.value_linear(data_input)\n",
        "\n",
        "        rotated_queries = torch.bmm(queries, self.rotation_matrix)\n",
        "        rotated_keys = torch.bmm(keys, self.rotation_matrix)\n",
        "\n",
        "        attention_scores = torch.bmm(rotated_queries, rotated_keys.transpose(1, 2))\n",
        "        attention_scores = attention_scores / math.sqrt(embed_dim)\n",
        "\n",
        "        if return_attention:\n",
        "            return attention_scores\n",
        "\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        attention_output = torch.bmm(attention_weights, values)\n",
        "        attention_output = attention_output.transpose(1, 2).reshape(batch_size, seq_len, embed_dim)\n",
        "\n",
        "        return attention_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CvtM5G5DjV_I"
      },
      "outputs": [],
      "source": [
        "# Multi-head attention module\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super().__init__()\n",
        "        self.params = params\n",
        "        # [INSTRUCTION] Initialize multiple attention heads.\n",
        "        # Create a ModuleList of AttentionHeadWithRoPE instances, one for each attention head specified in 'params'.\n",
        "        pass  # Initialize attention heads here\n",
        "\n",
        "        # [INSTRUCTION] Define an output linear transformation that combines the outputs of all heads.\n",
        "        # This linear layer should project from the concatenated dimension back to the embedding size.\n",
        "        pass  # Initialize output linear transformation here\n",
        "\n",
        "        # [INSTRUCTION] Add dropout for regularization after combining the outputs of the attention heads.\n",
        "        pass  # Initialize dropout layer here\n",
        "\n",
        "        embed_dim = params['embed_dim']\n",
        "        num_heads = params['num_heads']\n",
        "\n",
        "        self.attention_heads = nn.ModuleList([AttentionHeadWithRoPE(params) for _ in range(num_heads)])\n",
        "\n",
        "        self.output_linear = nn.Linear(embed_dim * num_heads, embed_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(params['dropout'])\n",
        "\n",
        "    def forward(self, data_input):\n",
        "        # [INSTRUCTION] Compute the output for each attention head and concatenate them along the last dimension.\n",
        "        pass  # Compute and concatenate attention head outputs here\n",
        "\n",
        "        # [INSTRUCTION] Apply the output linear transformation and dropout to the concatenated output.\n",
        "        pass  # Apply linear transformation and dropout here\n",
        "\n",
        "        head_outputs = [head(data_input) for head in self.attention_heads]\n",
        "\n",
        "        attention_output = torch.cat(head_outputs, dim=-1)\n",
        "\n",
        "        output = self.dropout(self.output_linear(attention_output))\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_dPfT-R6jGAf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Swish-Gated Linear Unit (SiGLU) activation function\n",
        "class SwishGLU(nn.Module):\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.linear_gate = nn.Linear(size, size)\n",
        "        self.linear = nn.Linear(size, size)\n",
        "        self.beta = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, data_input):\n",
        "        # [INSTRUCTION] Implement the SwishGLU activation function.\n",
        "        # The function should use a gated mechanism with a swish activation to modulate the input data.\n",
        "        pass  # Implement your activation function here\n",
        "\n",
        "        gate = torch.sigmoid(self.linear_gate(data_input)) * data_input\n",
        "        activation = self.linear(data_input)\n",
        "        output = gate + self.beta * activation\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "B0tOMYGWjGDX"
      },
      "outputs": [],
      "source": [
        "# Transformer block consisting of attention and feedforward layers\n",
        "class LLamaBlock(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super().__init__()\n",
        "        self.params = params\n",
        "        # [INSTRUCTION] Define the RMS normalization layers 'norm1' and 'norm2'.\n",
        "        # Each should be initialized with appropriate dimensions based on sequence length and embedding size.\n",
        "        pass  # Initialize RMSNorm instances here\n",
        "\n",
        "        # [INSTRUCTION] Initialize the attention head using the AttentionHeadWithRoPE class with given parameters.\n",
        "        pass  # Initialize the attention head here\n",
        "\n",
        "        # [INSTRUCTION] Define a feedforward network consisting of a linear layer followed by a SwishGLU activation.\n",
        "        # Ensure the dimensionality matches the embedding size defined in params.\n",
        "        pass  # Initialize the feedforward network here\n",
        "\n",
        "        self.norm1 = RMSNorm(params['embed_dim'])\n",
        "        self.norm2 = RMSNorm(params['embed_dim'])\n",
        "\n",
        "        self.attention = MultiHeadAttention(params)\n",
        "\n",
        "        self.ff_linear1 = nn.Linear(params['embed_dim'], params['embed_dim'])\n",
        "        self.ff_activation = SwishGLU(params['embed_dim'])\n",
        "        self.ff_linear2 = nn.Linear(params['embed_dim'], params['embed_dim'])\n",
        "\n",
        "    def forward(self, data_input):\n",
        "        # [INSTRUCTION] Implement the forward pass integrating normalization, attention, and feedforward layers.\n",
        "        # Include residual connections for the attention and feedforward blocks.\n",
        "        # Hint: Remember to apply normalization before and after the attention, as well as before the feedforward network.\n",
        "        pass  # Implement the forward logic with residuals and integration of components\n",
        "\n",
        "        norm1_output = self.norm1(data_input)\n",
        "\n",
        "        attention_output = self.attention(norm1_output)\n",
        "        attention_output = data_input + attention_output \n",
        "\n",
        "        norm2_output = self.norm2(attention_output)\n",
        "\n",
        "        ff1_output = self.ff_linear1(norm2_output)\n",
        "        ff_activation_output = self.ff_activation(ff1_output)\n",
        "        ff_output = self.ff_linear2(ff_activation_output)\n",
        "        output = attention_output + ff_output  \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WrhhoIYmjGF4"
      },
      "outputs": [],
      "source": [
        "# LLaMA-like language model\n",
        "class LLama(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super().__init__()\n",
        "        self.params = params\n",
        "        # [INSTRUCTION] Initialize the embedding layer for the input tokens.\n",
        "        pass  # Initialize embedding layer here\n",
        "\n",
        "        # [INSTRUCTION] Construct a sequence of transformer blocks using OrderedDict.\n",
        "        # Use a loop to create a specified number of LLamaBlock instances based on 'transformer_blocks' in params.\n",
        "        pass  # Create transformer blocks here\n",
        "\n",
        "        # [INSTRUCTION] Define the final linear transformation layers.\n",
        "        # This should include a linear layer to match the embedding dimension, a SwishGLU activation, and a final linear layer to map back to the vocabulary size.\n",
        "        pass  # Initialize the final linear transformation layers here\n",
        "\n",
        "        # Optionally print the total number of parameters in the model\n",
        "        self.embedding = nn.Embedding(params['vocab_size'], params['embed_dim'])\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleDict()\n",
        "        for i in range(params['transformer_blocks']):\n",
        "            block_name = f\"block_{i}\"\n",
        "            self.transformer_blocks[block_name] = LLamaBlock(params)\n",
        "\n",
        "        self.final_linear1 = nn.Linear(params['embed_dim'], params['ff_dim'])\n",
        "        self.final_activation = SwishGLU(params['ff_dim'])\n",
        "        self.final_linear2 = nn.Linear(params['ff_dim'], params['vocab_size'])\n",
        "\n",
        "        print(\"Model parameters:\", sum([p.numel() for p in self.parameters()]))\n",
        "\n",
        "    def forward(self, input_ids, target_ids=None):\n",
        "        # [INSTRUCTION] Implement the forward pass of the model.\n",
        "        # Embed the input_ids, process them through the LLama_blocks, and apply the final linear transformations.\n",
        "        # If target_ids are provided, compute and return the cross-entropy loss along with the logits.\n",
        "        pass  # Implement forward logic and loss computation here\n",
        "\n",
        "        embed = self.embedding(input_ids)\n",
        "\n",
        "        for block_name, block in self.transformer_blocks.items():\n",
        "            embed = block(embed)\n",
        "\n",
        "        logits = self.final_linear2(self.final_activation(self.final_linear1(embed)))\n",
        "\n",
        "        loss = None\n",
        "        if target_ids is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
        "\n",
        "        return logits, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XN7dKtlOqD5p"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing function\n",
        "def prepare_data(file_path):\n",
        "    # [INSTRUCTION] Open the file and read the text. Then create a list of unique characters and build two dictionaries:\n",
        "    # one to convert characters to indices and another from indices to characters.\n",
        "    pass  # Implement code to read file, create character list, and build dictionaries here\n",
        "    # [INSTRUCTION] Convert the entire text into a tensor of indices corresponding to the characters.\n",
        "    pass  # Convert characters in the text to indices and create a tensor here\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    unique_chars = list(set(text))\n",
        "    char_to_idx = {char: idx for idx, char in enumerate(unique_chars)}\n",
        "    idx_to_char = {idx: char for idx, char in enumerate(unique_chars)}\n",
        "    vocabulary_size = len(unique_chars)\n",
        "\n",
        "    data_tensor = torch.tensor([char_to_idx[char] for char in text])\n",
        "\n",
        "    return data_tensor, vocabulary_size, char_to_idx, idx_to_char\n",
        "\n",
        "# Function to create training batches\n",
        "def batch_generator(data, split, batch_size, seq_len, params):\n",
        "    # [INSTRUCTION] Split the data into training, validation, and test sets based on the provided ratios.\n",
        "    # Consider using 80% of data for training, 10% for validation, and 10% for testing.\n",
        "    pass  # Split the data into the appropriate segments here\n",
        "    pass  # Select data for the current training or evaluation phase here\n",
        "    # [INSTRUCTION] Generate random starting points for each batch and construct input and target sequences.\n",
        "    # The target sequence for each input is simply the input sequence shifted by one position.\n",
        "    pass  # Generate random batch start indices and construct input and target sequences here\n",
        "    train_size = int(0.8 * len(data))\n",
        "    val_size = int(0.1 * len(data))\n",
        "    test_size = len(data) - train_size - val_size\n",
        "\n",
        "    if split == 'train':\n",
        "        dataset = data[:train_size]\n",
        "    elif split == 'validation':\n",
        "        dataset = data[train_size:train_size + val_size]\n",
        "    else:\n",
        "        dataset = data[train_size + val_size:]\n",
        "\n",
        "    batch_start_indices = torch.randint(0, len(dataset) - seq_len - 1, (batch_size,))\n",
        "    input_sequences = []\n",
        "    target_sequences = []\n",
        "    for start_idx in batch_start_indices:\n",
        "        input_sequence = dataset[start_idx:start_idx + seq_len]\n",
        "        target_sequence = dataset[start_idx + 1:start_idx + seq_len + 1]\n",
        "        input_sequences.append(input_sequence)\n",
        "        target_sequences.append(target_sequence)\n",
        "\n",
        "    input_sequences = torch.stack(input_sequences)\n",
        "    target_sequences = torch.stack(target_sequences)\n",
        "\n",
        "    return input_sequences, target_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FsCcVecxmZS9"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate model loss\n",
        "@torch.no_grad()\n",
        "def eval_loss(model, dataset, params):\n",
        "    results = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"validation\"]:\n",
        "        batch_losses = []\n",
        "        for _ in range(10):\n",
        "            input_batch, target_batch = batch_generator(dataset, split, params['training_batch'], params['sequence_length'], params)\n",
        "            input_batch = input_batch.to(device)\n",
        "            target_batch = target_batch.to(device)\n",
        "            _, batch_loss = model(input_batch, target_batch)\n",
        "            batch_losses.append(batch_loss.item())\n",
        "        results[split] = np.mean(batch_losses)\n",
        "    model.train()\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0XmhO4cGmiKe"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "import time\n",
        "import pandas as pd\n",
        "def train_model(model, optimizer, dataset, params, scheduler=None):\n",
        "    all_losses = []\n",
        "    start = time.time()\n",
        "    for epoch in range(params['training_epochs']):\n",
        "        optimizer.zero_grad()\n",
        "        input_batch, target_batch = batch_generator(dataset, 'train', params['training_batch'], params['sequence_length'], params)\n",
        "\n",
        "        input_batch = input_batch.to(device)\n",
        "        target_batch = target_batch.to(device)\n",
        "        _, batch_loss = model(input_batch, target_batch)\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        if (epoch + 1) % params['logging_frequency'] == 0:\n",
        "            time_elapsed = time.time() - start\n",
        "            evaluation_result = eval_loss(model, dataset, params)\n",
        "            all_losses.append(evaluation_result)\n",
        "            print(\n",
        "                f\"Epoch {epoch + 1}/{params['training_epochs']} | \"\n",
        "                f\"Validation Loss: {evaluation_result['validation']:.4f} | \"\n",
        "                f\"Time: {time_elapsed:.2f}s\"\n",
        "            )\n",
        "            start = time.time()\n",
        "            if scheduler:\n",
        "                print(\"lr: \", scheduler.get_lr())\n",
        "\n",
        "    print(\"Final Validation Loss: \", all_losses[-1]['validation'])\n",
        "    return pd.DataFrame(all_losses).plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "ZzTD03mmmkYr",
        "outputId": "ae4747b1-56cf-48ab-e3be-d157bd26970b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU not found, using CPU for training.\n",
            "Model parameters: 3617250\n",
            "Epoch 1/100 | Validation Loss: 4.2033 | Time: 0.69s\n",
            "Epoch 2/100 | Validation Loss: 3.7869 | Time: 0.63s\n",
            "Epoch 3/100 | Validation Loss: 3.9715 | Time: 0.57s\n",
            "Epoch 4/100 | Validation Loss: 3.4610 | Time: 0.58s\n",
            "Epoch 5/100 | Validation Loss: 3.5068 | Time: 0.52s\n",
            "Epoch 6/100 | Validation Loss: 3.3957 | Time: 0.52s\n",
            "Epoch 7/100 | Validation Loss: 3.2429 | Time: 0.51s\n",
            "Epoch 8/100 | Validation Loss: 3.0799 | Time: 0.52s\n",
            "Epoch 9/100 | Validation Loss: 3.0626 | Time: 0.53s\n",
            "Epoch 10/100 | Validation Loss: 3.0409 | Time: 0.52s\n",
            "Epoch 11/100 | Validation Loss: 2.9892 | Time: 0.55s\n",
            "Epoch 12/100 | Validation Loss: 2.9229 | Time: 0.52s\n",
            "Epoch 13/100 | Validation Loss: 2.9040 | Time: 0.52s\n",
            "Epoch 14/100 | Validation Loss: 2.8817 | Time: 0.52s\n",
            "Epoch 15/100 | Validation Loss: 2.8524 | Time: 0.53s\n",
            "Epoch 16/100 | Validation Loss: 2.8345 | Time: 0.61s\n",
            "Epoch 17/100 | Validation Loss: 2.7967 | Time: 0.52s\n",
            "Epoch 18/100 | Validation Loss: 2.7674 | Time: 0.51s\n",
            "Epoch 19/100 | Validation Loss: 2.7712 | Time: 0.53s\n",
            "Epoch 20/100 | Validation Loss: 2.7623 | Time: 0.51s\n",
            "Epoch 21/100 | Validation Loss: 2.7440 | Time: 0.51s\n",
            "Epoch 22/100 | Validation Loss: 2.7228 | Time: 0.53s\n",
            "Epoch 23/100 | Validation Loss: 2.7064 | Time: 0.51s\n",
            "Epoch 24/100 | Validation Loss: 2.6976 | Time: 0.52s\n",
            "Epoch 25/100 | Validation Loss: 2.6649 | Time: 0.52s\n",
            "Epoch 26/100 | Validation Loss: 2.6664 | Time: 0.53s\n",
            "Epoch 27/100 | Validation Loss: 2.6644 | Time: 0.51s\n",
            "Epoch 28/100 | Validation Loss: 2.6395 | Time: 0.57s\n",
            "Epoch 29/100 | Validation Loss: 2.6323 | Time: 0.53s\n",
            "Epoch 30/100 | Validation Loss: 2.6243 | Time: 0.51s\n",
            "Epoch 31/100 | Validation Loss: 2.6224 | Time: 0.54s\n",
            "Epoch 32/100 | Validation Loss: 2.5969 | Time: 0.52s\n",
            "Epoch 33/100 | Validation Loss: 2.5936 | Time: 0.54s\n",
            "Epoch 34/100 | Validation Loss: 2.5885 | Time: 0.55s\n",
            "Epoch 35/100 | Validation Loss: 2.5682 | Time: 0.54s\n",
            "Epoch 36/100 | Validation Loss: 2.5711 | Time: 0.51s\n",
            "Epoch 37/100 | Validation Loss: 2.5393 | Time: 0.54s\n",
            "Epoch 38/100 | Validation Loss: 2.5412 | Time: 0.51s\n",
            "Epoch 39/100 | Validation Loss: 2.5275 | Time: 0.52s\n",
            "Epoch 40/100 | Validation Loss: 2.5097 | Time: 0.59s\n",
            "Epoch 41/100 | Validation Loss: 2.5123 | Time: 0.53s\n",
            "Epoch 42/100 | Validation Loss: 2.4861 | Time: 0.53s\n",
            "Epoch 43/100 | Validation Loss: 2.4846 | Time: 0.54s\n",
            "Epoch 44/100 | Validation Loss: 2.4935 | Time: 0.53s\n",
            "Epoch 45/100 | Validation Loss: 2.4648 | Time: 0.54s\n",
            "Epoch 46/100 | Validation Loss: 2.4633 | Time: 0.54s\n",
            "Epoch 47/100 | Validation Loss: 2.4485 | Time: 0.53s\n",
            "Epoch 48/100 | Validation Loss: 2.4426 | Time: 0.54s\n",
            "Epoch 49/100 | Validation Loss: 2.4004 | Time: 0.56s\n",
            "Epoch 50/100 | Validation Loss: 2.4153 | Time: 0.52s\n",
            "Epoch 51/100 | Validation Loss: 2.3791 | Time: 0.55s\n",
            "Epoch 52/100 | Validation Loss: 2.3946 | Time: 0.54s\n",
            "Epoch 53/100 | Validation Loss: 2.3354 | Time: 0.52s\n",
            "Epoch 54/100 | Validation Loss: 2.3517 | Time: 0.54s\n",
            "Epoch 55/100 | Validation Loss: 2.3425 | Time: 0.53s\n",
            "Epoch 56/100 | Validation Loss: 2.3197 | Time: 0.53s\n",
            "Epoch 57/100 | Validation Loss: 2.3026 | Time: 0.54s\n",
            "Epoch 58/100 | Validation Loss: 2.2889 | Time: 0.55s\n",
            "Epoch 59/100 | Validation Loss: 2.2577 | Time: 0.54s\n",
            "Epoch 60/100 | Validation Loss: 2.2466 | Time: 0.54s\n",
            "Epoch 61/100 | Validation Loss: 2.2218 | Time: 0.54s\n",
            "Epoch 62/100 | Validation Loss: 2.2187 | Time: 0.55s\n",
            "Epoch 63/100 | Validation Loss: 2.1955 | Time: 0.60s\n",
            "Epoch 64/100 | Validation Loss: 2.1477 | Time: 0.54s\n",
            "Epoch 65/100 | Validation Loss: 2.1379 | Time: 0.54s\n",
            "Epoch 66/100 | Validation Loss: 2.1148 | Time: 0.54s\n",
            "Epoch 67/100 | Validation Loss: 2.1241 | Time: 0.54s\n",
            "Epoch 68/100 | Validation Loss: 2.0848 | Time: 0.55s\n",
            "Epoch 69/100 | Validation Loss: 2.0768 | Time: 0.55s\n",
            "Epoch 70/100 | Validation Loss: 2.0482 | Time: 0.54s\n",
            "Epoch 71/100 | Validation Loss: 2.0321 | Time: 0.54s\n",
            "Epoch 72/100 | Validation Loss: 1.9940 | Time: 0.56s\n",
            "Epoch 73/100 | Validation Loss: 1.9940 | Time: 0.55s\n",
            "Epoch 74/100 | Validation Loss: 1.9901 | Time: 0.54s\n",
            "Epoch 75/100 | Validation Loss: 1.9591 | Time: 0.57s\n",
            "Epoch 76/100 | Validation Loss: 1.9397 | Time: 0.57s\n",
            "Epoch 77/100 | Validation Loss: 1.9494 | Time: 0.59s\n",
            "Epoch 78/100 | Validation Loss: 1.8917 | Time: 0.55s\n",
            "Epoch 79/100 | Validation Loss: 1.8609 | Time: 0.54s\n",
            "Epoch 80/100 | Validation Loss: 1.8593 | Time: 0.54s\n",
            "Epoch 81/100 | Validation Loss: 1.8450 | Time: 0.52s\n",
            "Epoch 82/100 | Validation Loss: 1.8047 | Time: 0.56s\n",
            "Epoch 83/100 | Validation Loss: 1.7846 | Time: 0.54s\n",
            "Epoch 84/100 | Validation Loss: 1.7784 | Time: 0.54s\n",
            "Epoch 85/100 | Validation Loss: 1.7737 | Time: 0.53s\n",
            "Epoch 86/100 | Validation Loss: 1.7192 | Time: 0.60s\n",
            "Epoch 87/100 | Validation Loss: 1.7164 | Time: 0.61s\n",
            "Epoch 88/100 | Validation Loss: 1.6971 | Time: 0.55s\n",
            "Epoch 89/100 | Validation Loss: 1.6693 | Time: 0.54s\n",
            "Epoch 90/100 | Validation Loss: 1.6676 | Time: 0.55s\n",
            "Epoch 91/100 | Validation Loss: 1.6180 | Time: 0.55s\n",
            "Epoch 92/100 | Validation Loss: 1.6267 | Time: 0.55s\n",
            "Epoch 93/100 | Validation Loss: 1.6121 | Time: 0.53s\n",
            "Epoch 94/100 | Validation Loss: 1.6074 | Time: 0.56s\n",
            "Epoch 95/100 | Validation Loss: 1.5814 | Time: 0.56s\n",
            "Epoch 96/100 | Validation Loss: 1.5456 | Time: 0.55s\n",
            "Epoch 97/100 | Validation Loss: 1.5680 | Time: 0.55s\n",
            "Epoch 98/100 | Validation Loss: 1.5188 | Time: 0.55s\n",
            "Epoch 99/100 | Validation Loss: 1.5066 | Time: 0.54s\n",
            "Epoch 100/100 | Validation Loss: 1.4885 | Time: 0.54s\n",
            "Final Validation Loss:  1.4885035037994385\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "cannot unpack non-iterable Axes object",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m train_model(model, optimizer, dataset, MODEL_PARAMS)\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable Axes object"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data_path = \"/Users/zihadeev/Downloads/input.txt\"\n",
        "dataset, vocab_size, char_to_idx, idx_to_char = prepare_data(data_path)\n",
        "\n",
        "MODEL_PARAMS = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'embed_dim': 128,\n",
        "    'sequence_length': 64,\n",
        "    'training_batch': 64,\n",
        "    'training_epochs': 100,\n",
        "    'logging_frequency': 1,\n",
        "    'num_heads': 8,\n",
        "    'transformer_blocks': 6,\n",
        "    'ff_dim': 128,\n",
        "    'dropout': 0.1,\n",
        "    'rotation_matrix': generate_rotation_matrix(64, 128),\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(\"Using GPU for training.\")\n",
        "    MODEL_PARAMS['device'] = 'cuda'\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"GPU not found, using CPU for training.\")\n",
        "    MODEL_PARAMS['device'] = 'cpu'\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Distributing training across {torch.cuda.device_count()} GPUs.\")\n",
        "    model = nn.DataParallel(LLama(MODEL_PARAMS)).to(device)\n",
        "else:\n",
        "    model = LLama(MODEL_PARAMS).to(device)\n",
        "\n",
        "dataset = dataset.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "_, _ = train_model(model, optimizer, dataset, MODEL_PARAMS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "f285U_Amw0MU"
      },
      "outputs": [],
      "source": [
        "MODEL_PARAMS = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'embed_dim': 256,             # Increased embedding dimension\n",
        "    'sequence_length': 64,\n",
        "    'training_batch': 64,\n",
        "    'training_epochs': 100,\n",
        "    'logging_frequency': 1,\n",
        "    'num_heads': 16,              # Increased number of attention heads\n",
        "    'transformer_blocks': 12,      # Increased number of transformer blocks\n",
        "    'ff_dim': 128,\n",
        "    'dropout': 0.1,\n",
        "    'rotation_matrix': generate_rotation_matrix(64, 256),  # Adjusted rotation matrix dimensions\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK-_hOD5xJCp",
        "outputId": "1ceb65da-7819-4b11-9e6d-c9ca522c46b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original number of parameters: 3486592\n"
          ]
        }
      ],
      "source": [
        "# Original configuration\n",
        "embed_dim = 128\n",
        "num_heads = 8\n",
        "transformer_blocks = 6\n",
        "\n",
        "original_params = (\n",
        "    vocab_size * embed_dim +\n",
        "\n",
        "    transformer_blocks * (\n",
        "        2 * embed_dim +\n",
        "\n",
        "        num_heads * (\n",
        "            3 * embed_dim * embed_dim +\n",
        "\n",
        "            embed_dim * embed_dim\n",
        "        ) +\n",
        "\n",
        "        3 * embed_dim * embed_dim\n",
        "    ) +\n",
        "\n",
        "    2 * embed_dim * 128 \n",
        ")\n",
        "\n",
        "print(\"Original number of parameters:\", original_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfM_1JmtxLuc",
        "outputId": "1e001bb7-96c3-415e-d45a-ffb7682df883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated number of parameters: 52785920\n"
          ]
        }
      ],
      "source": [
        "embed_dim = 256\n",
        "num_heads = 16\n",
        "transformer_blocks = 12\n",
        "\n",
        "updated_params = (\n",
        "    vocab_size * embed_dim +\n",
        "\n",
        "    transformer_blocks * (\n",
        "        2 * embed_dim +\n",
        "\n",
        "        num_heads * (\n",
        "            3 * embed_dim * embed_dim +\n",
        "\n",
        "            embed_dim * embed_dim\n",
        "        ) +\n",
        "\n",
        "        3 * embed_dim * embed_dim\n",
        "    ) +\n",
        "\n",
        "    2 * embed_dim * 128  \n",
        ")\n",
        "\n",
        "print(\"Updated number of parameters:\", updated_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "jDhIKrjlxgA8",
        "outputId": "dfb6da9e-9dbb-475a-9277-d6e0911bb565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU not found, using CPU for training.\n",
            "Model parameters: 53599848\n",
            "Epoch 1/100 | Validation Loss: 4.2892 | Time: 5.97s\n",
            "Epoch 2/100 | Validation Loss: 4.2632 | Time: 5.82s\n",
            "Epoch 3/100 | Validation Loss: 3.8099 | Time: 4.87s\n",
            "Epoch 4/100 | Validation Loss: 3.6838 | Time: 5.03s\n",
            "Epoch 5/100 | Validation Loss: 3.7284 | Time: 6.37s\n",
            "Epoch 6/100 | Validation Loss: 3.5935 | Time: 5.91s\n",
            "Epoch 7/100 | Validation Loss: 3.4153 | Time: 6.61s\n",
            "Epoch 8/100 | Validation Loss: 3.1729 | Time: 5.03s\n",
            "Epoch 9/100 | Validation Loss: 3.0512 | Time: 4.96s\n",
            "Epoch 10/100 | Validation Loss: 2.9873 | Time: 5.03s\n",
            "Epoch 11/100 | Validation Loss: 2.9548 | Time: 5.58s\n",
            "Epoch 12/100 | Validation Loss: 3.0905 | Time: 5.28s\n",
            "Epoch 13/100 | Validation Loss: 3.1540 | Time: 5.50s\n",
            "Epoch 14/100 | Validation Loss: 2.9198 | Time: 5.21s\n",
            "Epoch 15/100 | Validation Loss: 2.8767 | Time: 5.06s\n",
            "Epoch 16/100 | Validation Loss: 2.8783 | Time: 4.99s\n",
            "Epoch 17/100 | Validation Loss: 2.7710 | Time: 5.08s\n",
            "Epoch 18/100 | Validation Loss: 2.8073 | Time: 6.19s\n",
            "Epoch 19/100 | Validation Loss: 2.8445 | Time: 5.28s\n",
            "Epoch 20/100 | Validation Loss: 2.9620 | Time: 5.16s\n",
            "Epoch 21/100 | Validation Loss: 2.7969 | Time: 5.31s\n",
            "Epoch 22/100 | Validation Loss: 2.7607 | Time: 5.34s\n",
            "Epoch 23/100 | Validation Loss: 2.6714 | Time: 5.36s\n",
            "Epoch 24/100 | Validation Loss: 2.6544 | Time: 5.19s\n",
            "Epoch 25/100 | Validation Loss: 2.6448 | Time: 5.18s\n",
            "Epoch 26/100 | Validation Loss: 2.6735 | Time: 5.23s\n",
            "Epoch 27/100 | Validation Loss: 2.6151 | Time: 5.29s\n",
            "Epoch 28/100 | Validation Loss: 2.5867 | Time: 5.44s\n",
            "Epoch 29/100 | Validation Loss: 2.5542 | Time: 5.26s\n",
            "Epoch 30/100 | Validation Loss: 2.5505 | Time: 5.42s\n",
            "Epoch 31/100 | Validation Loss: 2.5205 | Time: 5.22s\n",
            "Epoch 32/100 | Validation Loss: 2.4981 | Time: 5.36s\n",
            "Epoch 33/100 | Validation Loss: 2.4795 | Time: 5.61s\n",
            "Epoch 34/100 | Validation Loss: 2.4708 | Time: 5.25s\n",
            "Epoch 35/100 | Validation Loss: 2.4346 | Time: 5.15s\n",
            "Epoch 36/100 | Validation Loss: 2.4055 | Time: 5.39s\n",
            "Epoch 37/100 | Validation Loss: 2.3653 | Time: 5.43s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_PARAMS\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[10], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, dataset, params, scheduler)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogging_frequency\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m     time_elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m---> 22\u001b[0m     evaluation_result \u001b[38;5;241m=\u001b[39m \u001b[43meval_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     all_losses\u001b[38;5;241m.\u001b[39mappend(evaluation_result)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluation_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_elapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m     )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36meval_loss\u001b[0;34m(model, dataset, params)\u001b[0m\n\u001b[1;32m     10\u001b[0m     input_batch \u001b[38;5;241m=\u001b[39m input_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     target_batch \u001b[38;5;241m=\u001b[39m target_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 12\u001b[0m     _, batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     batch_losses\u001b[38;5;241m.\u001b[39mappend(batch_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     14\u001b[0m results[split] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(batch_losses)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[7], line 44\u001b[0m, in \u001b[0;36mLLama.forward\u001b[0;34m(self, input_ids, target_ids)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Process through LLama_blocks\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block_name, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 44\u001b[0m     embed \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Apply final linear transformations\u001b[39;00m\n\u001b[1;32m     47\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_linear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_activation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_linear1(embed)))\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[6], line 37\u001b[0m, in \u001b[0;36mLLamaBlock.forward\u001b[0;34m(self, data_input)\u001b[0m\n\u001b[1;32m     34\u001b[0m norm1_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(data_input)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Multi-head attention with residual connection\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm1_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m data_input \u001b[38;5;241m+\u001b[39m attention_output  \u001b[38;5;66;03m# Residual connection\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Apply RMS normalization after attention\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[4], line 36\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, data_input)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# [INSTRUCTION] Apply the output linear transformation and dropout to the concatenated output.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Apply linear transformation and dropout here\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m head_outputs \u001b[38;5;241m=\u001b[39m [\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_input\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_heads]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Concatenate attention head outputs along the last dimension\u001b[39;00m\n\u001b[1;32m     39\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(head_outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[3], line 47\u001b[0m, in \u001b[0;36mAttentionHeadWithRoPE.forward\u001b[0;34m(self, data_input, return_attention)\u001b[0m\n\u001b[1;32m     44\u001b[0m rotated_keys \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(keys, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotation_matrix)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Compute scaled dot-product attention\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrotated_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotated_keys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(embed_dim)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_attention:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(\"Using GPU for training.\")\n",
        "    MODEL_PARAMS['device'] = 'cuda'\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"GPU not found, using CPU for training.\")\n",
        "    MODEL_PARAMS['device'] = 'cpu'\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Distributing training across {torch.cuda.device_count()} GPUs.\")\n",
        "    model = nn.DataParallel(LLama(MODEL_PARAMS)).to(device)\n",
        "else:\n",
        "    model = LLama(MODEL_PARAMS).to(device)\n",
        "\n",
        "dataset = dataset.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "_, _ = train_model(model, optimizer, dataset, MODEL_PARAMS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We increased the number of parameters and adjusted them. It changed the validation loss, but I can't see that much of a difference. Only can say that the time that it took to train the model has risen significantly. For the more accurate and precise results we need more computational resources. I think that increasing the number of model's praramters is not enough to achieve now a better performance. We need more resources and time for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wr9ZKk90zJLP"
      },
      "outputs": [],
      "source": [
        "class AttentionHeadWithRoPE(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super().__init__()\n",
        "        self.params = params\n",
        "        embed_dim = params['embed_dim']\n",
        "\n",
        "        self.query_linear = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.key_linear = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.value_linear = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "        self.register_buffer('query_cache', None)\n",
        "        self.register_buffer('key_cache', None)\n",
        "        self.register_buffer('value_cache', None)\n",
        "\n",
        "    def forward(self, data_input, return_attention=False):\n",
        "        batch_size, seq_len, embed_dim = data_input.shape\n",
        "\n",
        "        if self.query_cache is None:\n",
        "            self.query_cache = self.query_linear(data_input)\n",
        "        if self.key_cache is None:\n",
        "            self.key_cache = self.key_linear(data_input)\n",
        "        if self.value_cache is None:\n",
        "            self.value_cache = self.value_linear(data_input)\n",
        "\n",
        "        attention_scores = torch.bmm(self.query_cache, self.key_cache.transpose(1, 2))\n",
        "        attention_scores = attention_scores / math.sqrt(embed_dim)\n",
        "\n",
        "        if return_attention:\n",
        "            return attention_scores\n",
        "\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        attention_output = torch.bmm(attention_weights, self.value_cache)\n",
        "        attention_output = attention_output.transpose(1, 2).reshape(batch_size, seq_len, embed_dim)\n",
        "\n",
        "        return attention_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "p9uztYsPzWK6"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super().__init__()\n",
        "        self.params = params\n",
        "        embed_dim = params['embed_dim']\n",
        "        num_heads = params['num_heads']\n",
        "        self.heads_per_group = params.get('heads_per_group', num_heads)\n",
        "\n",
        "        # Initialize attention heads and KV caches\n",
        "        self.attention_heads = nn.ModuleList([AttentionHeadWithRoPE(params) for _ in range(num_heads)])\n",
        "        self.output_linear = nn.Linear(embed_dim * num_heads, embed_dim)\n",
        "        self.dropout = nn.Dropout(params['dropout'])\n",
        "\n",
        "    def forward(self, data_input):\n",
        "        batch_size, seq_len, embed_dim = data_input.shape\n",
        "\n",
        "        # Group attention heads into smaller groups\n",
        "        grouped_heads = [self.attention_heads[i:i+self.heads_per_group] for i in range(0, len(self.attention_heads), self.heads_per_group)]\n",
        "\n",
        "        head_outputs = []\n",
        "        for group in grouped_heads:\n",
        "            group_output = torch.cat([head(data_input) for head in group], dim=-1)\n",
        "            head_outputs.append(group_output)\n",
        "\n",
        "        attention_output = torch.cat(head_outputs, dim=-1)\n",
        "        output = self.dropout(self.output_linear(attention_output))\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4jnVLQN1eQZ",
        "outputId": "95c5b25a-0d99-4fcb-ccfa-fbacb52e6e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 53599848\n",
            "Epoch 1/100 | Validation Loss: 4.0328 | Time: 5.40s\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train the model with augmented dataset\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmented_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_PARAMS\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[10], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, dataset, params, scheduler)\u001b[0m\n\u001b[1;32m     12\u001b[0m target_batch \u001b[38;5;241m=\u001b[39m target_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m _, batch_loss \u001b[38;5;241m=\u001b[39m model(input_batch, target_batch)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scheduler:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ],
      "source": [
        "data_path = \"/Users/zihadeev/Downloads/input.txt\"\n",
        "dataset, vocab_size, char_to_idx, idx_to_char = prepare_data(data_path)\n",
        "\n",
        "augmented_dataset = torch.cat([dataset, dataset])\n",
        "\n",
        "MODEL_PARAMS['vocab_size'] = vocab_size\n",
        "MODEL_PARAMS['sequence_length'] = 64  \n",
        "MODEL_PARAMS['dataset_size'] = len(augmented_dataset)\n",
        "\n",
        "model = LLama(MODEL_PARAMS).to(device)\n",
        "augmented_dataset = augmented_dataset.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "_, _ = train_model(model, optimizer, augmented_dataset, MODEL_PARAMS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "RX7o0wNI1jjg",
        "outputId": "d1981f56-8598-4881-97b5-2692398c364c"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "string indices must be integers, not 'str'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Before enhancements\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mevaluate_and_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_PARAMS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# After enhancements\u001b[39;00m\n\u001b[1;32m     16\u001b[0m evaluate_and_plot(model, augmented_dataset, MODEL_PARAMS)\n",
            "Cell \u001b[0;32mIn[20], line 4\u001b[0m, in \u001b[0;36mevaluate_and_plot\u001b[0;34m(model, dataset, params)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_and_plot\u001b[39m(model, dataset, params):\n\u001b[1;32m      3\u001b[0m     results \u001b[38;5;241m=\u001b[39m eval_loss(model, dataset, params)\n\u001b[0;32m----> 4\u001b[0m     validation_losses \u001b[38;5;241m=\u001b[39m [\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[1;32m      5\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(validation_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
          ]
        }
      ],
      "source": [
        "def evaluate_and_plot(model, dataset, params):\n",
        "    results = eval_loss(model, dataset, params)\n",
        "    validation_losses = [result['validation'] for result in results]\n",
        "    plt.plot(validation_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Validation Loss over Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "evaluate_and_plot(model, dataset, MODEL_PARAMS)\n",
        "\n",
        "evaluate_and_plot(model, augmented_dataset, MODEL_PARAMS)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
