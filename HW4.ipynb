{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "jkZwM-KyYj4L"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Model"
      ],
      "metadata": {
        "id": "Q7VDjAcyYplR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the model\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21u2HgzRYo-7",
        "outputId": "cc0fd085-9feb-4084-ed40-2aa9359eba62"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Freezing Model Parameters"
      ],
      "metadata": {
        "id": "HayhA2zkY1J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# freezing model parameters\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.data = param.data.to(torch.float32)"
      ],
      "metadata": {
        "id": "TqAstpn3Y6ef"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enable Gradient Checkpointing"
      ],
      "metadata": {
        "id": "Gn0RwIwzZdr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enable gradient checkpointing\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "model.transformer.wte.weight.requires_grad = True\n",
        "model.transformer.wpe.weight.requires_grad = True"
      ],
      "metadata": {
        "id": "BrhGjryDZitA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Output Casting"
      ],
      "metadata": {
        "id": "0i5nbQygaH6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLMHead(nn.Module):\n",
        "    def __init__(self, original_lm_head):\n",
        "        super(CustomLMHead, self).__init__()\n",
        "        self.original_lm_head = original_lm_head\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        output = self.original_lm_head(*args, **kwargs)\n",
        "        return output.to(torch.float32)\n",
        "\n",
        "# Replace the modelâ€™s lm head with an instance of this custom class\n",
        "model.lm_head = CustomLMHead(model.lm_head)"
      ],
      "metadata": {
        "id": "1OVR5avVaJGj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Function"
      ],
      "metadata": {
        "id": "NAVeJ_dOaYOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Function to Print Trainable Parameters\n",
        "def print_trainable_parameters(model):\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Number of trainable parameters: {trainable_params}\")\n",
        "\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUZdNOBFab7v",
        "outputId": "98047711-72a2-49ca-c860-b61ef09183c0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters: 39383808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Creation"
      ],
      "metadata": {
        "id": "ve8ge-o9aoES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to format context, question, and answer into a prompt template\n",
        "def format_prompt(context, question, answer=None):\n",
        "    if answer:\n",
        "        return f\"Context: {context}\\nQuestion: {question}\\nAnswer: {answer}\"\n",
        "    else:\n",
        "        return f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
        "\n",
        "# Mock QA dataset\n",
        "qa_dataset = [\n",
        "    {\"context\": \"The sky is blue.\", \"question\": \"What color is the sky?\", \"answer\": \"Blue\"},\n",
        "    {\"context\": \"The cat is on the roof.\", \"question\": \"Where is the cat?\", \"answer\": \"On the roof\"},\n",
        "    {\"context\": \"The car is red.\", \"question\": \"What color is the car?\", \"answer\": \"Red\"},\n",
        "    {\"context\": \"The dog is brown.\", \"question\": \"What color is the dog?\", \"answer\": \"Brown\"},\n",
        "]\n",
        "\n",
        "# Map the QA dataset to this prompt format using the tokenizer.\n",
        "def map_qa_to_prompt(qa_dataset, tokenizer):\n",
        "    prompts = []\n",
        "    for qa in qa_dataset:\n",
        "        prompt = format_prompt(qa['context'], qa['question'], qa['answer'])\n",
        "        inputs = tokenizer(prompt, return_tensors='pt')\n",
        "        prompts.append(inputs)\n",
        "    return prompts\n",
        "\n",
        "# Tokenize the QA dataset\n",
        "tokenized_prompts = map_qa_to_prompt(qa_dataset, tokenizer)\n",
        "for prompt in tokenized_prompts:\n",
        "    print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaKW2cy_asDd",
        "outputId": "f08c20a2-34e6-435d-e163-4734561656f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[21947,    25,   383,  6766,   318,  4171,    13,   198, 24361,    25,\n",
            "          1867,  3124,   318,   262,  6766,    30,   198, 33706,    25,  4518]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "{'input_ids': tensor([[21947,    25,   383,  3797,   318,   319,   262,  9753,    13,   198,\n",
            "         24361,    25,  6350,   318,   262,  3797,    30,   198, 33706,    25,\n",
            "          1550,   262,  9753]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "{'input_ids': tensor([[21947,    25,   383,  1097,   318,  2266,    13,   198, 24361,    25,\n",
            "          1867,  3124,   318,   262,  1097,    30,   198, 33706,    25,  2297]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "{'input_ids': tensor([[21947,    25,   383,  3290,   318,  7586,    13,   198, 24361,    25,\n",
            "          1867,  3124,   318,   262,  3290,    30,   198, 33706,    25,  4373]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "l1iCif-_b2wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs = 3\n",
        "learning_rate = 5e-5\n",
        "\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
        "total_steps = len(tokenized_prompts) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    for batch in tokenized_prompts:\n",
        "        inputs = batch['input_ids']\n",
        "        labels = batch['input_ids']\n",
        "        outputs = model(inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpCw5-cNb4QS",
        "outputId": "08b53c75-28a2-417a-e426-9a4497fe39b4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.798947811126709\n",
            "Epoch 1, Loss: 2.9669504165649414\n",
            "Epoch 1, Loss: 3.3174118995666504\n",
            "Epoch 1, Loss: 3.0745296478271484\n",
            "Epoch 2, Loss: 2.761045455932617\n",
            "Epoch 2, Loss: 2.8329732418060303\n",
            "Epoch 2, Loss: 3.0522823333740234\n",
            "Epoch 2, Loss: 3.178790807723999\n",
            "Epoch 3, Loss: 2.59621262550354\n",
            "Epoch 3, Loss: 2.6281633377075195\n",
            "Epoch 3, Loss: 3.1056389808654785\n",
            "Epoch 3, Loss: 3.6082780361175537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading LoRA Model"
      ],
      "metadata": {
        "id": "0tFkJBqkckyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJvTRqFAhtqo",
        "outputId": "0c8c510f-8e6d-402f-f87d-1d3e589ed56e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.4)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.32.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=64,\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,  # Conventional\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGaapLL8cmiy",
        "outputId": "2f96b759-3f71-4cd5-b398-cc5c72c79f2b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/layer.py:1119: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Function"
      ],
      "metadata": {
        "id": "JG3_KShIdyGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inference function to generate answers based on a given context and question\n",
        "def generate_answer(model, tokenizer, context, question):\n",
        "    prompt = format_prompt(context, question)\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(inputs['input_ids'], max_length=50)\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "07290g0Sd3Fe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Inference"
      ],
      "metadata": {
        "id": "3k9jSjZReGd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the inference function with sample contexts and questions.\n",
        "sample_context = \"The sky is blue.\"\n",
        "sample_question = \"What color is the sky?\"\n",
        "answer = generate_answer(model, tokenizer, sample_context, sample_question)\n",
        "print(f\"Context: {sample_context}\\nQuestion: {sample_question}\\nAnswer: {answer}\")\n",
        "\n",
        "sample_context = \"The cat is on the roof.\"\n",
        "sample_question = \"Where is the cat?\"\n",
        "answer = generate_answer(model, tokenizer, sample_context, sample_question)\n",
        "print(f\"Context: {sample_context}\\nQuestion: {sample_question}\\nAnswer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgUfN4-YeKHK",
        "outputId": "2fa9b80d-d11d-471c-eefd-410c30d88823"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: The sky is blue.\n",
            "Question: What color is the sky?\n",
            "Answer: Context: The sky is blue.\n",
            "Question: What color is the sky?\n",
            "Answer: The sky is blue.\n",
            "Question: What color is the sky?\n",
            "Question: The sky is blue.\n",
            "Question: What color is the sky?\n",
            "Context: The cat is on the roof.\n",
            "Question: Where is the cat?\n",
            "Answer: Context: The cat is on the roof.\n",
            "Question: Where is the cat?\n",
            "Answer: The cat is on the roof.\n",
            "Question: What is the cat?Answer: The cat is on the roof.\n",
            "Question: What is the\n"
          ]
        }
      ]
    }
  ]
}